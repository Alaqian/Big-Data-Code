{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a11e5d-b62b-4b17-b253-59d45e80f727",
   "metadata": {},
   "source": [
    "# Assignment 2 - Spark Dataframes\n",
    "***Note***: All the dataset files were stored in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda4be81-c73e-459b-a426-209b94e4631b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T21:37:02.470301Z",
     "iopub.status.busy": "2023-03-09T21:37:02.470301Z",
     "iopub.status.idle": "2023-03-09T21:37:11.492292Z",
     "shell.execute_reply": "2023-03-09T21:37:11.491695Z",
     "shell.execute_reply.started": "2023-03-09T21:37:02.470301Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.121.18.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fcda8046d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b389af-e9a2-4c6a-8c08-7fdb347fbac6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 15 Points\n",
    "**Datafile**: BreadBasket_DMS.csv\n",
    "\n",
    "**Solve**: What is the most popular (most sold) between the 8:00AM and 8:59AM for each day?\n",
    "\n",
    "Example output (not actual solution)\n",
    "\n",
    "    2016-10-30, Pastry\n",
    "\n",
    "    2016-10-31, Coffee\n",
    "     :\n",
    "     :\n",
    "\n",
    "### Approach:\n",
    "1. Import `BreadBasket_DMS.csv` into a dataframe\n",
    "2. Extract dates in `YYYY-MM-DD` format from the `Date` column and times in `hh:mm:ss` format from the `Time` column\n",
    "3. Filter the data by `Time` in the range of `08:00:00` and `08:59:00` inclusive and remove rows with `None` in the `Item` column\n",
    "4. Group the data by `Date` and `Item`, aggregate the `sum` of `Transaction` for each `Item` aliased as `Total` and, sort by `Date` and `Total`\n",
    "5. Group the data by `Date` and return the last `Item` and last `Total`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798a4acf-2cd2-43d5-90f4-91593e1139e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T21:37:11.495315Z",
     "iopub.status.busy": "2023-03-09T21:37:11.495315Z",
     "iopub.status.idle": "2023-03-09T21:37:20.919524Z",
     "shell.execute_reply": "2023-03-09T21:37:20.919524Z",
     "shell.execute_reply.started": "2023-03-09T21:37:11.495315Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the most popular (most sold) items between the 8:00 AM and 8:59 AM for each day and their total transactions that day:\n",
      "+----------+------------------+------------------+\n",
      "|      Date|Most Popular Iteam|Total Transactions|\n",
      "+----------+------------------+------------------+\n",
      "|2016-10-31|             Bread|                 2|\n",
      "|2016-11-01|               Tea|                 3|\n",
      "|2016-11-02|            Coffee|                 8|\n",
      "|2016-11-03|            Coffee|                 4|\n",
      "|2016-11-04|            Coffee|                 2|\n",
      "|2016-11-05|             Bread|                 6|\n",
      "|2016-11-07|            Coffee|                 1|\n",
      "|2016-11-08|            Coffee|                 1|\n",
      "|2016-11-09|            Coffee|                 1|\n",
      "|2016-11-10|            Coffee|                 2|\n",
      "|2016-11-11|             Bread|                 6|\n",
      "|2016-11-12|         Medialuna|                 1|\n",
      "|2016-11-14|            Coffee|                 2|\n",
      "|2016-11-15|  Keeping It Local|                 1|\n",
      "|2016-11-16|             Bread|                 1|\n",
      "|2016-11-17|          Siblings|                 2|\n",
      "|2016-11-18|            Coffee|                 6|\n",
      "|2016-11-19|             Bread|                 3|\n",
      "|2016-11-21|            Coffee|                 2|\n",
      "|2016-11-22|         Medialuna|                 1|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Import BreadBasket_DMS.csv into a dataframe and `filter` out rows with `NONE` in the `Item` column\n",
    "from pyspark.sql.functions import col\n",
    "BreadBasket_DMS = spark.read.option(\"header\", True).option(\"InferSchema\", True).csv(\"BreadBasket_DMS.csv\")\n",
    "BreadBasket_DMS = BreadBasket_DMS.filter(col(\"Item\") != \"NONE\")\n",
    "\n",
    "# 2. Extract dates in `YYYY-MM-DD` format from the `Date` column and times in `hh:mm:ss` format from the `Time` column\n",
    "from pyspark.sql.functions import to_date, date_format\n",
    "BreadBasket_DMS = BreadBasket_DMS.withColumn(\"Date\", to_date(col(\"Date\"), \"YYYY-MM-DD\"))\n",
    "BreadBasket_DMS = BreadBasket_DMS.withColumn(\"Time\", date_format(col(\"Time\"),\"hh:mm:ss\"))\n",
    "\n",
    "# 3. Filter the data by `Time` in the range of `08:00:00` and `08:59:00` inclusive \n",
    "q1 = BreadBasket_DMS\n",
    "q1 = q1.filter((col(\"Time\") <= \"08:59:00\") & (col(\"Time\") >= \"08:00:00\"))\n",
    "\n",
    "# 4. Group the data by `Date` and `Item`, aggregate by the `count` and, sort by `Date` and `count`\n",
    "q1 = q1.groupBy(\"Date\",\"Item\").count().sort(\"Date\",\"count\")\n",
    "\n",
    "# 5. Group the data by `Date` and return the last `Item` and last `count`\n",
    "from pyspark.sql.functions import last\n",
    "q1 = q1.groupBy(\"Date\").agg(last(\"Item\").alias(\"Most Popular Iteam\"),last(\"count\").alias(\"Total Transactions\"))\n",
    "\n",
    "# Display results\n",
    "print(\"List of the most popular (most sold) items between the 8:00 AM and 8:59 AM for each day and their total transactions that day:\")\n",
    "q1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5749546-f3a2-46b4-8210-c86fa7c87502",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. 15 Points\n",
    "**Datafile**: BreadBasket_DMS.csv\n",
    "\n",
    "**Solve**: What is the most common item bought along with “Brownie”? (items bought in the same transaction)\n",
    "\n",
    "### Assumptions:\n",
    "We will assume that we will count each time an item was bought with “Brownie”. If an item was bought more than once in the same transaction we will count each time that item was bought in that transaction.\n",
    "\n",
    "### Approach:\n",
    "1. Import `BreadBasket_DMS.csv` into a dataframe\n",
    "2. Extract dates in `YYYY-MM-DD` format from the `Date` column and times in `hh:mm:ss` format from the `Time` column\n",
    "3. Make list of Brownie Trans and Dates and Time\n",
    "4. Filter Brownie from list\n",
    "5. Join both\n",
    "6. GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90daa6a0-13db-4f5f-a13f-796a2df13589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T21:37:20.919524Z",
     "iopub.status.busy": "2023-03-09T21:37:20.919524Z",
     "iopub.status.idle": "2023-03-09T21:37:22.730152Z",
     "shell.execute_reply": "2023-03-09T21:37:22.728957Z",
     "shell.execute_reply.started": "2023-03-09T21:37:20.919524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of the most common items bought along with “Brownie” sorted by their counts\n",
      "+-----------------+-----+\n",
      "|             Item|count|\n",
      "+-----------------+-----+\n",
      "|           Coffee|  237|\n",
      "|            Bread|  115|\n",
      "|              Tea|   71|\n",
      "|             Cake|   43|\n",
      "|    Hot chocolate|   42|\n",
      "|         Sandwich|   27|\n",
      "|        Alfajores|   27|\n",
      "|          Cookies|   26|\n",
      "|            Juice|   24|\n",
      "|           Pastry|   23|\n",
      "|        Medialuna|   19|\n",
      "|           Muffin|   18|\n",
      "|             Soup|   15|\n",
      "|            Scone|   12|\n",
      "|             Coke|   11|\n",
      "|       Farm House|   11|\n",
      "|         Truffles|   11|\n",
      "|    Mineral water|    9|\n",
      "|            Toast|    7|\n",
      "|Hearty & Seasonal|    6|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Import BreadBasket_DMS.csv into a dataframe and `filter` out rows with `NONE` in the `Item` column\n",
    "from pyspark.sql.functions import col\n",
    "BreadBasket_DMS = spark.read.option(\"header\", True).option(\"InferSchema\", True).csv(\"BreadBasket_DMS.csv\")\n",
    "BreadBasket_DMS = BreadBasket_DMS.filter(col(\"Item\") != \"NONE\")\n",
    "\n",
    "# 2. Extract dates in `YYYY-MM-DD` format from the `Date` column and times in `hh:mm:ss` format from the `Time` column\n",
    "from pyspark.sql.functions import to_date, date_format\n",
    "BreadBasket_DMS = BreadBasket_DMS.withColumn(\"Date\", to_date(col(\"Date\"), \"YYYY-MM-DD\"))\n",
    "BreadBasket_DMS = BreadBasket_DMS.withColumn(\"Time\", date_format(col(\"Time\"),\"hh:mm:ss\"))\n",
    "\n",
    "# 3 Brownie Transactions Date and Time\n",
    "BrownieTransactions = BreadBasket_DMS.filter(col(\"Item\") == \"Brownie\").sort(\"Transaction\")\n",
    "\n",
    "# Non-Brownie Transactions.sort(\"Transaction\")\n",
    "OtherTransactions = BreadBasket_DMS.filter(col(\"Item\") != \"Brownie\")\n",
    "\n",
    "# Items bought with Brownies\n",
    "JoinExpression = BrownieTransactions[\"Transaction\"] == OtherTransactions[\"Transaction\"]\n",
    "ItemBougtWithBrownie = OtherTransactions.join(BrownieTransactions,JoinExpression, \"left_semi\").sort(\"Transaction\")\n",
    "\n",
    "# Total Transactions\n",
    "from pyspark.sql.functions import desc\n",
    "print(\"List of the most common items bought along with “Brownie” sorted by their counts\")\n",
    "ItemBougtWithBrownie.groupBy(\"Item\").count().sort(desc(\"count\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
