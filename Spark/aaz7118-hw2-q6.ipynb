{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a11e5d-b62b-4b17-b253-59d45e80f727",
   "metadata": {},
   "source": [
    "# Assignment 2 - Spark Dataframes\n",
    "***Note***: All the dataset files were stored in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda4be81-c73e-459b-a426-209b94e4631b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T22:16:38.298524Z",
     "iopub.status.busy": "2023-03-09T22:16:38.298524Z",
     "iopub.status.idle": "2023-03-09T22:16:46.075989Z",
     "shell.execute_reply": "2023-03-09T22:16:46.075989Z",
     "shell.execute_reply.started": "2023-03-09T22:16:38.298524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.121.18.24:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23432860e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10611afc-6218-4bb3-933f-a48e44cdd764",
   "metadata": {},
   "source": [
    "# 6. 15 Points\n",
    "**Dataset**: romeo-juliet-pg1777.txt\n",
    "\n",
    "**Solve**: Do **word count** in pyspark.\n",
    "Ignore punctuation, and normalize to lower case. Accept only the characters in this set: **[0-9a-zA-Z]**\n",
    "\n",
    "## Discussion\n",
    "\n",
    "For the purpose of this assignment, all characters not in this set: [a-z, A-Z, 0-9] were replaced with spaces or `\" \"` using the following `regex_string`:\n",
    "```python\n",
    "regex_string = r\"[^a-zA-Z0-9]\"\n",
    "```\n",
    "However, there are are issues with this and one such issue is discussed below:\n",
    "### Hypenated and Apostrophe Words:\n",
    "The regex used would mistreat characters with hyphens or apostrophes such as `Don't` or `Mother-in-law` into `[\"don\", \"t\"]` and `[\"mother\", \"in\", \"law\"]`. \n",
    "### Proposed Alternative Regex:\n",
    "A more appropriate regex to include hyphenated and appsotrophe words might be given by the two options below. (source: https://stackoverflow.com/questions/27715581/):\n",
    "```python\n",
    "re.findall(r\"(?!'.*')\\b[\\w'-]+\\b\", line.lower())\n",
    "```\n",
    "or\n",
    "```python\n",
    "re.findall(r\"[A-Za-z0-9]+(?:[-'][A-Za-z0-9]+)*\", line.lower())\n",
    "```\n",
    "However, this would also include possessive words such as `professor's` which might be undesired.\n",
    "\n",
    "### Approach:\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcba088a-f3a4-4eaf-8160-32fca7de3860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T22:16:46.075989Z",
     "iopub.status.busy": "2023-03-09T22:16:46.075989Z",
     "iopub.status.idle": "2023-03-09T22:16:53.429744Z",
     "shell.execute_reply": "2023-03-09T22:16:53.428206Z",
     "shell.execute_reply.started": "2023-03-09T22:16:46.075989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|     those|   17|\n",
      "|  carnegie|   10|\n",
      "|      some|   58|\n",
      "|      chor|    2|\n",
      "|       art|   55|\n",
      "|     still|   15|\n",
      "|   nourish|    1|\n",
      "|     cures|    1|\n",
      "| solemnity|    3|\n",
      "|     feign|    1|\n",
      "|    imagin|    1|\n",
      "|consortest|    1|\n",
      "|   pitcher|    1|\n",
      "|      earl|    1|\n",
      "|      hope|    4|\n",
      "|    shroud|    3|\n",
      "|    unfirm|    1|\n",
      "|   embrace|    1|\n",
      "|     often|    4|\n",
      "|  received|    3|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, split, explode, desc\n",
    "# read\n",
    "RomeoJuliet = spark.read.text(\"romeo-juliet-pg1777.txt\")\n",
    "WordCount = RomeoJuliet\n",
    "\n",
    "# replace words not in [0-9a-zA-Z] with \" \"\n",
    "regex_string = r\"[^a-zA-Z0-9]\"\n",
    "WordCount = WordCount.select(regexp_replace(\"value\", regex_string, \" \").alias(\"LINE\"))\n",
    "\n",
    "# lower case\n",
    "WordCount = WordCount.select(lower(\"LINE\").alias(\"line\"))\n",
    "\n",
    "# split words in line\n",
    "WordCount = WordCount.select(split(\"line\", \" \").alias(\"words_in_line\"))\n",
    "\n",
    "# explode words\n",
    "WordCount = WordCount.select(explode(\"words_in_line\").alias(\"word\"))\n",
    "\n",
    "# group by word and count\n",
    "WordCount = WordCount.groupBy(\"word\").count()\n",
    "\n",
    "# Display\n",
    "WordCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71643758-fa08-4c86-82bd-19ce73c863a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T21:35:55.684022Z",
     "iopub.status.busy": "2023-03-08T21:35:55.684022Z",
     "iopub.status.idle": "2023-03-08T21:35:56.520640Z",
     "shell.execute_reply": "2023-03-08T21:35:56.519640Z",
     "shell.execute_reply.started": "2023-03-08T21:35:55.684022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, udf, col\n",
    "from pyspark.sql.types import BooleanType\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "Restaurants = spark.read\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".option(\"delimiter\", ';')\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"multiline\", True)\\\n",
    ".csv(\"Restaurants_in_Durham_County_NC.csv\")\n",
    "\n",
    "RestaurantsQ7 = Restaurants.filter((col(\"status\") == \"ACTIVE\") & (col(\"rpt_area_desc\") == \"Food Service\"))\n",
    "# Split geolocation - cast lat and lng to double\n",
    "RestaurantsQ7 = RestaurantsQ7.select(\"Premise_Name\", (split(\"geolocation\", \", \"))[0].cast(\"double\").alias(\"lat\"), (split(\"geolocation\", \", \"))[1].cast(\"double\").alias(\"lng\")).dropna()\n",
    "\n",
    "Foreclosure = spark.read.json(\"durham-nc-foreclosure-2006-2016.json\").select(\"fields.geocode\").dropna()\n",
    "\n",
    "joinExpression = udf(lambda lat, lng, geocode: haversine((lat, lng), geocode, unit='mi') <= 1, BooleanType())\n",
    "q7 = RestaurantsQ7.join(Foreclosure, joinExpression(RestaurantsQ7.lat, RestaurantsQ7.lng, Foreclosure.geocode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61cc6e83-ef63-4cdb-ac5c-e474691b6f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T21:45:06.030673Z",
     "iopub.status.busy": "2023-03-08T21:45:06.030673Z",
     "iopub.status.idle": "2023-03-08T21:45:06.318638Z",
     "shell.execute_reply": "2023-03-08T21:45:06.318638Z",
     "shell.execute_reply.started": "2023-03-08T21:45:06.030673Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.types import BooleanType\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "Restaurants = spark.read \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"delimiter\", ';') \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"multiline\", True) \\\n",
    "    .csv(\"Restaurants_in_Durham_County_NC.csv\")\n",
    "RestaurantsQ7 = Restaurants.filter((col(\"status\") == \"ACTIVE\") & (col(\"rpt_area_desc\") == \"Food Service\"))\n",
    "RestaurantsQ7 = RestaurantsQ7.select(\"Premise_Name\",split(\"geolocation\", \", \")[0].cast(\"double\").alias(\"lat\"),split(\"geolocation\", \", \")[1].cast(\"double\").alias(\"lng\")).dropna()                    .dropna()\n",
    "\n",
    "Foreclosure = spark.read.json(\"durham-nc-foreclosure-2006-2016.json\").select(\"fields.geocode\").dropna()\n",
    "\n",
    "def distance(lat, lng, geocode):\n",
    "    return haversine((lat, lng), geocode, unit='mi') <= 1\n",
    "\n",
    "distance_udf = udf(distance, BooleanType())\n",
    "q7 = RestaurantsQ7.join(Foreclosure, distance_udf(RestaurantsQ7[\"lat\"], RestaurantsQ7[\"lng\"], Foreclosure[\"geocode\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b31f597-f537-4d16-8588-8c5c57232480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T21:45:06.497417Z",
     "iopub.status.busy": "2023-03-08T21:45:06.497417Z",
     "iopub.status.idle": "2023-03-08T21:45:14.728542Z",
     "shell.execute_reply": "2023-03-08T21:45:14.728542Z",
     "shell.execute_reply.started": "2023-03-08T21:45:06.497417Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63593"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a63454d-f3f2-4db5-9d53-ba560bb80a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T21:45:14.729541Z",
     "iopub.status.busy": "2023-03-08T21:45:14.729541Z",
     "iopub.status.idle": "2023-03-08T21:45:23.236457Z",
     "shell.execute_reply": "2023-03-08T21:45:23.235457Z",
     "shell.execute_reply.started": "2023-03-08T21:45:14.729541Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Premise_Name|count|\n",
      "+--------------------+-----+\n",
      "|(G&J) NOT JUST A ...|   81|\n",
      "|            80 FRESH|   81|\n",
      "|     A & D BUFFALO'S|   40|\n",
      "|  ACADEMY QUICK STOP|    9|\n",
      "|AI FUJI JAPANESE ...|   16|\n",
      "|              AKASHI|    2|\n",
      "|AL-TAIBA HALAL MA...|   53|\n",
      "|ALAKSHA'S CUSTOM ...|   81|\n",
      "|            ALIVIA'S|  115|\n",
      "|ALOFT DURHAM DOWN...|  247|\n",
      "|              ALPACA|   73|\n",
      "|        AMANTE PIZZA|   98|\n",
      "|   AMC SOUTHPOINT 17|   10|\n",
      "|       AMERICAN HERO|   75|\n",
      "|AMERICAN LEGION P...|  212|\n",
      "|   AMERICAN MELTDOWN|   10|\n",
      "|AMERICAN TOBACCO ...|  229|\n",
      "|    AMF DURHAM LANES|    5|\n",
      "|ANOTHER BROKEN EG...|    9|\n",
      "|AR- RAZAQ ISLAMIC...|   59|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q7.groupBy(\"Premise_Name\").count().sort(\"Premise_Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0741ad-62db-4654-b1fe-a595914fd55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
